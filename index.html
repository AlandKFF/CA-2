<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Chapter 2: Memory and Performance Concepts Summary</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            background-color: #f9f9f9;
            color: #333;
        }

        h1,
        h2 {
            text-align: center;
            color: #00539C;
        }

        h3 {
            color: #0074D9;
            margin-top: 40px;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background-color: #fff;
        }

        th,
        td {
            border: 1px solid #ccc;
            padding: 8px 12px;
            text-align: left;
        }

        th {
            background-color: #e2e2e2;
        }

        .section {
            margin-bottom: 40px;
        }
    </style>
</head>

<body>
    <nav>
    <ul style="list-style-type: none; padding: 0; text-align: center; background-color: #00539C; margin-bottom: 20px;">
        <li style="display: inline; margin-right: 20px;">
            <a href="chapter_1.html" style="color: white; text-decoration: none;">Chapter 1</a>
        </li>
        <li style="display: inline; margin-right: 20px;">
            <a href="#" style="color: white; text-decoration: none;">Chapter 2</a>
        </li>
        <li style="display: inline;">
            <a href="chapter_3.html" style="color: white; text-decoration: none;">Chapter 3</a>
        </li>
    </ul>
</nav>

    <h1>Chapter 2: Memory and Performance Concepts</h1>

    <!-- 1. Pipelining -->
    <div class="section">
        <h2>1. Pipelining</h2>
        <table>
            <tr>
                <th>Aspect</th>
                <th>Description</th>
            </tr>
            <tr>
                <td>Types</td>
                <td>Standard Pipelining, Super Pipelining, Superscalar Pipelining</td>
            </tr>
            <tr>
                <td>Stages</td>
                <td>Typically includes Fetch, Decode, Execute, and Write-back stages</td>
            </tr>
            <tr>
                <td>Hazards</td>
                <td>Pipelining hazards (data, control, and structural hazards) that can cause stalls or flushes</td>
            </tr>
            <tr>
                <td>Pros</td>
                <td>Increases instruction throughput; allows overlapping of instruction execution</td>
            </tr>
            <tr>
                <td>Cons</td>
                <td>Increased complexity, potential pipeline stalls, hazards requiring additional hardware support</td>
            </tr>
        </table>
    </div>

    <!-- 2. Definitions -->
    <div class="section">
        <h2>2. Key Performance Definitions</h2>
        <table>
            <tr>
                <th>Term</th>
                <th>Description</th>
            </tr>
            <tr>
                <td>Clock Cycle</td>
                <td>The basic time unit of a processor; the interval between two pulses of the oscillator.</td>
            </tr>
            <tr>
                <td>Latency</td>
                <td>The time it takes for an instruction to complete (or for a memory access to occur).</td>
            </tr>
            <tr>
                <td>CPI (Clock Cycles Per Instruction)</td>
                <td>The average number of clock cycles each instruction takes to execute.</td>
            </tr>
            <tr>
                <td>Throughput</td>
                <td>The number of instructions completed per unit of time.</td>
            </tr>
            <tr>
                <td>CPU Cycle</td>
                <td>The time required for an instruction to move one step in the pipeline; often equals one or more
                    clock cycles.</td>
            </tr>
        </table>
    </div>

    <!-- 3. Method of Accessing -->
    <div class="section">
        <h2>3. Memory Access Methods</h2>
        <table>
            <tr>
                <th>Access Method</th>
                <th>Description</th>
            </tr>
            <tr>
                <td>Sequential Access</td>
                <td>Data is accessed in a fixed, linear sequence; suitable for devices like magnetic tapes.</td>
            </tr>
            <tr>
                <td>Direct Access</td>
                <td>Data is retrieved by directly addressing its physical location; used in disk drives.</td>
            </tr>
            <tr>
                <td>Random Access</td>
                <td>Any memory location can be accessed directly and immediately; typical for RAM and cache.</td>
            </tr>
            <tr>
                <td>Associative Access</td>
                <td>Data is retrieved based on content matching rather than a fixed address; used in some cache systems.
                </td>
            </tr>
        </table>
    </div>

    <!-- 4. Memory Performance -->
    <div class="section">
        <h2>4. Memory Performance Parameters</h2>
        <table>
            <tr>
                <th>Parameter</th>
                <th>Description</th>
            </tr>
            <tr>
                <td>Access Time</td>
                <td>The time to perform a read or write operation after an address is provided.</td>
            </tr>
            <tr>
                <td>Memory Cycle Time</td>
                <td>Access time plus any additional time before another access can commence.</td>
            </tr>
            <tr>
                <td>Transfer Rate</td>
                <td>The rate at which data is transferred into or out of memory, often defined as 1/(cycle time) for
                    RAM.</td>
            </tr>
        </table>
    </div>

    <!-- 5. RAM -->
    <div class="section">
        <h2>5. RAM (Random Access Memory)</h2>
        <table>
            <tr>
                <th>Type</th>
                <th>Description</th>
            </tr>
            <tr>
                <td>SRAM</td>
                <td>Static RAM; faster and used as cache memory (L1, L2, L3), but more expensive and power-hungry.</td>
            </tr>
            <tr>
                <td>DRAM</td>
                <td>Dynamic RAM; used as main memory in personal computers; cheaper and denser but slower due to refresh
                    cycles.</td>
            </tr>
        </table>
    </div>

    <!-- 6. ROM -->
    <div class="section">
        <h2>6. ROM (Read-Only Memory)</h2>
        <table>
            <tr>
                <th>Type</th>
                <th>Description</th>
            </tr>
            <tr>
                <td>PROM</td>
                <td>Programmable ROM; can be written once using special equipment.</td>
            </tr>
            <tr>
                <td>Read-Mostly Memory</td>
                <td>
                    Includes several types:
                    <ul>
                        <li><strong>EPROM:</strong> Erasable with UV light, allowing multiple updates.</li>
                        <li><strong>EEPROM:</strong> Electrically erasable, supports byte-level updates.</li>
                        <li><strong>Flash Memory:</strong> A faster form of EEPROM that erases in blocks.</li>
                    </ul>
                </td>
            </tr>
        </table>
    </div>

    <!-- 7. Cache -->
    <div class="section">
        <h2>7. Cache Memory</h2>

        <!-- Cache Address -->
        <h3>Cache Address</h3>
        <table>
            <tr>
                <th>Type</th>
                <th>Description</th>
            </tr>
            <tr>
                <td>Physical Cache</td>
                <td>Stores data using physical addresses from main memory; accessed via the MMU.</td>
            </tr>
            <tr>
                <td>Logical (Virtual) Cache</td>
                <td>Stores data using virtual addresses; accessed directly by the processor without translation.</td>
            </tr>
        </table>

        <!-- Cache Size -->
        <h3>Cache Size</h3>
        <table>
            <tr>
                <th>Size</th>
                <th>Impact</th>
            </tr>
            <tr>
                <td>Small Cache</td>
                <td>Lower cost per bit and faster addressing, but may lead to a higher miss rate.</td>
            </tr>
            <tr>
                <td>Large Cache</td>
                <td>Increases hit ratio by storing more data but can be slightly slower and more expensive.</td>
            </tr>
        </table>

        <!-- Mapping Techniques -->
        <h3>Mapping Techniques</h3>
        <table>
            <tr>
                <th>Technique</th>
                <th>Description</th>
            </tr>
            <tr>
                <td>Direct Mapping</td>
                <td>Each memory block maps to a specific cache line; simple but prone to collisions.</td>
            </tr>
            <tr>
                <td>Associative Mapping</td>
                <td>Any memory block can be placed in any cache line; flexible but more complex and slower.</td>
            </tr>
            <tr>
                <td>Set-Associative Mapping</td>
                <td>Hybrid approach where cache is divided into sets; each block can be placed in any line within a set.
                </td>
            </tr>
        </table>

        <!-- Replacement Algorithms -->
        <h3>Replacement Algorithms</h3>
        <table>
            <tr>
                <th>Algorithm</th>
                <th>Description</th>
            </tr>
            <tr>
                <td>LRU (Least Recently Used)</td>
                <td>Replaces the block that hasn't been used for the longest time.</td>
            </tr>
            <tr>
                <td>FIFO (First-In-First-Out)</td>
                <td>Replaces the oldest block in the cache.</td>
            </tr>
            <tr>
                <td>LFU (Least Frequently Used)</td>
                <td>Replaces the block that has been referenced the fewest times.</td>
            </tr>
            <tr>
                <td>Random</td>
                <td>Selects a block at random to replace.</td>
            </tr>
        </table>

        <!-- Write Policy -->
        <h3>Write Policy</h3>
        <table>
            <tr>
                <th>Policy</th>
                <th>Description</th>
            </tr>
            <tr>
                <td>Write-Through</td>
                <td>All write operations update both the cache and main memory, ensuring consistency but at slower
                    speeds.</td>
            </tr>
            <tr>
                <td>Write-Back</td>
                <td>Updates are made only in the cache, and a dirty bit marks the line for later update to main memory,
                    improving performance but risking data loss on failure.</td>
            </tr>
        </table>

        <!-- Line Size -->
        <h3>Line Size</h3>
        <table>
            <tr>
                <th>Line Size</th>
                <th>Impact</th>
            </tr>
            <tr>
                <td>Smaller Line Size</td>
                <td>Minimizes wasted space but may reduce the hit ratio due to less adjacent data loaded.</td>
            </tr>
            <tr>
                <td>Larger Line Size</td>
                <td>Increases the hit ratio by loading more adjacent data (spatial locality) but can waste cache space
                    if unused.</td>
            </tr>
        </table>

        <!-- Cache Levels -->
        <h3>Cache Levels</h3>
        <table>
            <tr>
                <th>Cache Level</th>
                <th>Description</th>
            </tr>
            <tr>
                <td>Single-Level Cache (L1)</td>
                <td>On-chip cache that is very fast but has limited capacity.</td>
            </tr>
            <tr>
                <td>Multi-Level Cache (L1, L2, L3)</td>
                <td>
                    Multiple cache levels where L1 is fastest and smallest, L2 is larger and slower, and L3 is shared
                    among cores and offers the highest capacity.
                </td>
            </tr>
        </table>

        <!-- Unified vs. Split Cache -->
        <h3>Unified vs. Split Cache</h3>
        <table>
            <tr>
                <th>Type</th>
                <th>Description</th>
            </tr>
            <tr>
                <td>Unified Cache</td>
                <td>Stores both instructions and data in a single cache, offering flexibility but with potential
                    contention.</td>
            </tr>
            <tr>
                <td>Split Cache</td>
                <td>Separates the cache into an instruction cache and a data cache, reducing conflicts and allowing
                    parallel access.</td>
            </tr>
        </table>

    </div>

</body>

</html>
